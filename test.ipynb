{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93208175",
   "metadata": {},
   "source": [
    "# 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdac71",
   "metadata": {},
   "source": [
    "Let's start by importing all the needed packages and setting the function to get the `device`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b85494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AdversarialPromptGenerator import AdversarialPromptGenerator\n",
    "\n",
    "from integrated_gradients import integrated_gradients\n",
    "\n",
    "from our_base import LocalModel, HuggingFaceEmbeddings\n",
    "from our_token_shap import TokenizerSplitter, TokenSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # torch.backends.mps may not exist on all builds, guard with getattr\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af65aa7",
   "metadata": {},
   "source": [
    "# 2. Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf21d86",
   "metadata": {},
   "source": [
    "First, retrieve the Hugging Face Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "if not hf_api_key:\n",
    "    raise RuntimeError(\"Missing HUGGINGFACE_API_KEY. Set it in your environment or .env file.\")\n",
    "login(hf_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739100c4",
   "metadata": {},
   "source": [
    "# 3. TokenSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171c91e",
   "metadata": {},
   "source": [
    "Then, instantiate TokenSHAP using HuggingFace, specifically using the `meta-llama/Llama-3.2-1B-Instruct` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "local_model = LocalModel(model_name=model_path, max_new_tokens=1, temperature=None, device=DEVICE)\n",
    "hf_embedding = HuggingFaceEmbeddings(device=DEVICE)\n",
    "splitter = TokenizerSplitter(local_model.tokenizer)\n",
    "token_shap_local = TokenSHAP(model=local_model, splitter=splitter, vectorizer=hf_embedding, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defecde6",
   "metadata": {},
   "source": [
    "Instantiate the `PromptGenerator` to retrieve the adversarial prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_prompt_generator = AdversarialPromptGenerator()\n",
    "adversarial_suffix_path = \"./adv_suffixes.pt\" # tensor of all 100 suffixes\n",
    "all_prompts = adv_prompt_generator.get_from(adversarial_suffix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# tokenizer = local_model.tokenizer\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# model = local_model.model\n",
    "# # model = local_model\n",
    "# for prompt in all_prompts:\n",
    "# \tmessages = [\n",
    "# \t\t{\"role\": \"user\", \"content\": prompt},\n",
    "# \t]\n",
    "# \tinputs = tokenizer.apply_chat_template(\n",
    "# \t\tmessages,\n",
    "# \t\tadd_generation_prompt=True,\n",
    "# \t\ttokenize=True,\n",
    "# \t\treturn_dict=True,\n",
    "# \t\treturn_tensors=\"pt\",\n",
    "# \t).to(model.device)\n",
    "\n",
    "# \t# print(inputs)\n",
    "\n",
    "# \toutputs = model.generate(\n",
    "# \t\t**inputs,\n",
    "# \t\tmax_new_tokens=1,\n",
    "# \t\tdo_sample=False,\n",
    "# \t\ttemperature=None,\n",
    "# \t\ttop_p=None,\n",
    "# \t\tpad_token_id=tokenizer.eos_token_id\n",
    "# \t)\n",
    "# \tprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0833b",
   "metadata": {},
   "source": [
    "##### Test of Specific Functions of TokenSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_shap_local._calculate_baseline(all_prompts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2045c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_shap_local._get_result_per_combination(all_prompts[3], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local = token_shap_local.analyze(all_prompts[3], sampling_ratio=0.0)\n",
    "token_shap_local.print_colored_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad619d",
   "metadata": {},
   "source": [
    "##### Full Loop to Analyse All 100 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee786516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in all_prompts:\n",
    "    df_local = token_shap_local.analyze(prompt, sampling_ratio=0.0)\n",
    "    token_shap_local.print_colored_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b67ff",
   "metadata": {},
   "source": [
    "# 4. Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7286d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = integrated_gradients(\n",
    "    model=local_model.model,\n",
    "    tokenizer=local_model.tokenizer,\n",
    "    content=all_prompts[3],\n",
    "    steps=50,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "for tok, score in zip(result[\"tokens\"], result[\"attributions\"]):\n",
    "    print(f\"{tok:>10s} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Integrated Gradients (Captum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captum_integrated_gradients(model, tokenizer, content, device, steps=50):\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 1. Tokenize using chat template\n",
    "    prompt = [{\"role\": \"user\", \"content\": content}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Match user tokens for coherence with previous implementation\n",
    "    user_ids = tokenizer(\n",
    "        content,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"][0].to(device)\n",
    "\n",
    "    # Simple subsequence match\n",
    "    def find_subsequence(sequence, subseq):\n",
    "        for i in range(len(sequence) - len(subseq) + 1):\n",
    "            if torch.equal(sequence[i:i+len(subseq)], subseq):\n",
    "                return i, i + len(subseq)\n",
    "        return None, None\n",
    "\n",
    "    user_start, user_end = find_subsequence(input_ids[0], user_ids)\n",
    "\n",
    "    # 2. Identify target token (greedy)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        target_token_id = outputs.logits[0, -1].argmax().item()\n",
    "\n",
    "    # 3. Define forward function for Captum\n",
    "    # We need to compute gradients w.r.t embeddings, so we capture the embedding layer.\n",
    "    embed_layer = model.get_input_embeddings()\n",
    "\n",
    "    def forward_func(inputs_coords):\n",
    "        # LayerIntegratedGradients passes the output of the layer (embeddings) as the first argument\n",
    "        # We need to pass these embeddings to the model\n",
    "        # However, model() expects input_ids usually, but can take inputs_embeds\n",
    "        outputs = model(inputs_embeds=inputs_coords)\n",
    "        return outputs.logits[0, -1, target_token_id]\n",
    "\n",
    "    lig = LayerIntegratedGradients(forward_func, embed_layer)\n",
    "\n",
    "    # 4. Baselines\n",
    "    baseline_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    baseline_ids = torch.full_like(input_ids, baseline_token_id)\n",
    "\n",
    "    # 5. Attribute\n",
    "    # We pass input_ids to attribute. LIG will pass it to the layer, get embeddings, \n",
    "    # then pass embeddings to forward_func.\n",
    "    attributions = lig.attribute(inputs=input_ids,\n",
    "                                 baselines=baseline_ids,\n",
    "                                 n_steps=steps,\n",
    "                                 internal_batch_size=1)\n",
    "\n",
    "    # Sum over hidden dimension\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions.detach().cpu()\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Filter to user tokens if found\n",
    "    if user_start is not None:\n",
    "        tokens = tokens[user_start:user_end]\n",
    "        attributions = attributions[user_start:user_end]\n",
    "\n",
    "    return tokens, attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cap, attrs_cap = captum_integrated_gradients(\n",
    "    model=local_model.model,\n",
    "    tokenizer=local_model.tokenizer,\n",
    "    content=all_prompts[3],\n",
    "    device=DEVICE,\n",
    "    steps=50\n",
    ")\n",
    "\n",
    "print(\"Captum Integrated Gradients Results:\")\n",
    "for tok, score in zip(tokens_cap, attrs_cap):\n",
    "    print(f\"{tok:>10s} : {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}