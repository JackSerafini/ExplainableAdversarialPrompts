{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cdac71",
   "metadata": {},
   "source": [
    "##### Import the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b85494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AdversarialPromptGenerator import AdversarialPromptGenerator\n",
    "\n",
    "from our_base import LocalModel, HuggingFaceEmbeddings\n",
    "from our_token_shap import TokenizerSplitter, TokenSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # torch.backends.mps may not exist on all builds, guard with getattr\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf21d86",
   "metadata": {},
   "source": [
    "##### Retrieve the Hugging Face Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "if not hf_api_key:\n",
    "    raise RuntimeError(\"Missing HUGGINGFACE_API_KEY. Set it in your environment or .env file.\")\n",
    "login(hf_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171c91e",
   "metadata": {},
   "source": [
    "##### Instantiate TokenSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "local_model = LocalModel(model_name=model_path, max_new_tokens=1, temperature=None, device=DEVICE)\n",
    "hf_embedding = HuggingFaceEmbeddings(device=DEVICE)\n",
    "splitter = TokenizerSplitter(local_model.tokenizer)\n",
    "token_shap_local = TokenSHAP(model=local_model, splitter=splitter, vectorizer=hf_embedding, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defecde6",
   "metadata": {},
   "source": [
    "##### Instantiate PromptGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_prompt_generator = AdversarialPromptGenerator()\n",
    "adversarial_suffix_path = \"./adv_suffixes.pt\" # tensor of all 100 suffixes\n",
    "all_prompts = adv_prompt_generator.get_from(adversarial_suffix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer = splitter.tokenizer # it is literally the same, if not better (on device)\n",
    "# or local_model.tokenizer???\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# model = local_model\n",
    "for prompt in all_prompts:\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"user\", \"content\": prompt},\n",
    "\t]\n",
    "\tinputs = tokenizer.apply_chat_template(\n",
    "\t\tmessages,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\ttokenize=True,\n",
    "\t\treturn_dict=True,\n",
    "\t\treturn_tensors=\"pt\",\n",
    "\t).to(model.device)\n",
    "\n",
    "\t# print(inputs)\n",
    "\n",
    "\toutputs = model.generate(\n",
    "\t\t**inputs,\n",
    "\t\tmax_new_tokens=1,\n",
    "\t\tdo_sample=False,\n",
    "\t\ttemperature=None,\n",
    "\t\ttop_p=None,\n",
    "\t\tpad_token_id=tokenizer.eos_token_id\n",
    "\t)\n",
    "\tprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee786516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in all_prompts:\n",
    "    df_local = token_shap_local.analyze(prompt, sampling_ratio=0.0)\n",
    "    token_shap_local.print_colored_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
